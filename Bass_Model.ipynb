{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d458720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to data directory that contains the images\n",
    "datadir = '/Users/Will Hecmanczuk/OneDrive/Desktop/SBEVE/bass/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eb2581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5873a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts the (x lbs, x oz) format to x lbs\n",
    "def convert_to_pounds(pounds, ounces):\n",
    "    pounds = float(pounds)\n",
    "    ounces = float(ounces)\n",
    "    weight_in_pounds = pounds + (ounces / 16.0)\n",
    "    return weight_in_pounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dae32ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed IMG_3096.PNG to 0.PNG\n",
      "Renamed IMG_3098.PNG to 1.PNG\n",
      "Renamed IMG_3100.PNG to 2.PNG\n",
      "Renamed IMG_3101.PNG to 3.PNG\n",
      "Renamed IMG_3102.PNG to 4.PNG\n",
      "Renamed IMG_3103.PNG to 5.PNG\n",
      "Renamed IMG_3104.PNG to 6.PNG\n",
      "Renamed IMG_3105.PNG to 7.PNG\n",
      "Renamed IMG_3106.PNG to 8.PNG\n",
      "Renamed IMG_3107.PNG to 9.PNG\n",
      "Renamed IMG_3108.PNG to 10.PNG\n",
      "Renamed IMG_3109.PNG to 11.PNG\n",
      "Renamed IMG_3110.PNG to 12.PNG\n",
      "Renamed IMG_3111.PNG to 13.PNG\n",
      "Renamed IMG_3112.PNG to 14.PNG\n",
      "Renamed IMG_3113.PNG to 15.PNG\n",
      "Renamed IMG_3115.PNG to 16.PNG\n",
      "Renamed IMG_3116.PNG to 17.PNG\n",
      "Renamed IMG_3117.PNG to 18.PNG\n",
      "Renamed IMG_3118.PNG to 19.PNG\n",
      "Renamed IMG_3119.PNG to 20.PNG\n",
      "Renamed IMG_3120.PNG to 21.PNG\n",
      "Renamed IMG_3121.PNG to 22.PNG\n",
      "Renamed IMG_3122.PNG to 23.PNG\n",
      "Renamed IMG_3123.PNG to 24.PNG\n",
      "Renamed IMG_3124.PNG to 25.PNG\n",
      "Renamed IMG_3125.PNG to 26.PNG\n",
      "Renamed IMG_3126.PNG to 27.PNG\n",
      "Renamed IMG_3127.PNG to 28.PNG\n",
      "Renamed IMG_3128.PNG to 29.PNG\n",
      "Renamed IMG_3129.PNG to 30.PNG\n",
      "Renamed IMG_3130.PNG to 31.PNG\n",
      "Renamed IMG_3132.PNG to 32.PNG\n",
      "Renamed IMG_3133.PNG to 33.PNG\n",
      "Renamed IMG_3135.PNG to 34.PNG\n",
      "Renamed IMG_3137.PNG to 35.PNG\n",
      "Renamed IMG_3138.PNG to 36.PNG\n",
      "Renamed IMG_3139.PNG to 37.PNG\n",
      "Renamed IMG_3140.PNG to 38.PNG\n",
      "Renamed IMG_3141.PNG to 39.PNG\n",
      "Renamed IMG_3142.PNG to 40.PNG\n",
      "Renamed IMG_3143.PNG to 41.PNG\n",
      "Renamed IMG_3144.PNG to 42.PNG\n",
      "Renamed IMG_3145.PNG to 43.PNG\n",
      "Renamed IMG_3146.PNG to 44.PNG\n",
      "Renamed IMG_3147.PNG to 45.PNG\n",
      "Renamed IMG_3148.PNG to 46.PNG\n",
      "Renamed IMG_3149.PNG to 47.PNG\n",
      "Renamed IMG_3150.PNG to 48.PNG\n",
      "Renamed IMG_3151.PNG to 49.PNG\n",
      "Renamed IMG_3152.PNG to 50.PNG\n",
      "Renamed IMG_3153.PNG to 51.PNG\n",
      "Renamed IMG_3154.PNG to 52.PNG\n",
      "Renamed IMG_3155.PNG to 53.PNG\n",
      "Renamed IMG_3156.PNG to 54.PNG\n",
      "Renamed IMG_3157.PNG to 55.PNG\n",
      "Renamed IMG_3158.PNG to 56.PNG\n",
      "Renamed IMG_3159.PNG to 57.PNG\n",
      "Renamed IMG_3160.PNG to 58.PNG\n",
      "Renamed IMG_3161.PNG to 59.PNG\n",
      "Renamed IMG_3162.PNG to 60.PNG\n",
      "Renamed IMG_3163.PNG to 61.PNG\n",
      "Renamed IMG_3164.PNG to 62.PNG\n",
      "Renamed IMG_3165.PNG to 63.PNG\n",
      "Renamed IMG_3166.PNG to 64.PNG\n",
      "Renamed IMG_3167.PNG to 65.PNG\n",
      "Renamed IMG_3168.PNG to 66.PNG\n",
      "Renamed IMG_3169.PNG to 67.PNG\n",
      "Renamed IMG_3170.PNG to 68.PNG\n",
      "Renamed IMG_3171.PNG to 69.PNG\n",
      "Renamed IMG_3172.PNG to 70.PNG\n",
      "Renamed IMG_3173.PNG to 71.PNG\n",
      "Renamed IMG_3174.PNG to 72.PNG\n",
      "Renamed IMG_3175.PNG to 73.PNG\n",
      "Renamed IMG_3176.PNG to 74.PNG\n",
      "Renamed IMG_3177.PNG to 75.PNG\n",
      "Renamed IMG_3178.PNG to 76.PNG\n",
      "Renamed IMG_3179.PNG to 77.PNG\n",
      "Renamed IMG_3180.PNG to 78.PNG\n",
      "Renamed IMG_3181.PNG to 79.PNG\n",
      "Renamed IMG_3182.PNG to 80.PNG\n",
      "Renamed IMG_3183.PNG to 81.PNG\n",
      "Renamed IMG_3184.PNG to 82.PNG\n",
      "Renamed IMG_3185.PNG to 83.PNG\n",
      "Renamed IMG_3186.PNG to 84.PNG\n",
      "Renamed IMG_3187.PNG to 85.PNG\n",
      "Renamed IMG_3188.PNG to 86.PNG\n",
      "Renamed IMG_3189.PNG to 87.PNG\n",
      "Renamed IMG_3190.PNG to 88.PNG\n",
      "Renamed IMG_3191.PNG to 89.PNG\n",
      "Renamed IMG_3192.PNG to 90.PNG\n",
      "Renamed IMG_3193.PNG to 91.PNG\n",
      "Renamed IMG_3194.PNG to 92.PNG\n",
      "Renamed IMG_3195.PNG to 93.PNG\n",
      "Renamed IMG_3198.PNG to 94.PNG\n",
      "Renamed IMG_3199.PNG to 95.PNG\n",
      "Renamed IMG_3200.PNG to 96.PNG\n",
      "Renamed IMG_3201.PNG to 97.PNG\n",
      "Renamed IMG_3202.PNG to 98.PNG\n",
      "Renamed IMG_3203.PNG to 99.PNG\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# This function renames bass image files to x.png where x is an integer from 0-99\n",
    "def rename_files(directory):\n",
    "    files = os.listdir(directory)\n",
    "    for i, file_name in enumerate(sorted(files)):\n",
    "        file_ext = os.path.splitext(file_name)[1]\n",
    "        new_name = f\"{i}{file_ext}\"\n",
    "        old_path = os.path.join(directory, file_name)\n",
    "        new_path = os.path.join(directory, new_name)\n",
    "        os.rename(old_path, new_path)\n",
    "        print(f\"Renamed {file_name} to {new_name}\")\n",
    "\n",
    "# Specify the directory containing the files\n",
    "image_directory = datadir + \"bass_images/\"\n",
    "\n",
    "\n",
    "rename_files(image_directory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652f9c1d",
   "metadata": {},
   "source": [
    "# Below Is the first attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3452201c",
   "metadata": {},
   "source": [
    "# take 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6882d00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 2.2882, Valid Loss: 154.3598\n",
      "Epoch [2/10], Train Loss: 0.8839, Valid Loss: 37.5500\n",
      "Epoch [3/10], Train Loss: 0.6748, Valid Loss: 11.2727\n",
      "Epoch [4/10], Train Loss: 0.5774, Valid Loss: 4.6583\n",
      "Epoch [5/10], Train Loss: 0.5605, Valid Loss: 1.9849\n",
      "Epoch [6/10], Train Loss: 0.5117, Valid Loss: 1.6858\n",
      "Epoch [7/10], Train Loss: 0.4558, Valid Loss: 4.1749\n",
      "Epoch [8/10], Train Loss: 0.4300, Valid Loss: 2.2420\n",
      "Epoch [9/10], Train Loss: 0.3564, Valid Loss: 1.8533\n",
      "Epoch [10/10], Train Loss: 0.2714, Valid Loss: 1.9096\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import os\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define the model architecture\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 1)\n",
    "\n",
    "# Define the dataset class\n",
    "class BassDataset(Dataset):\n",
    "    def __init__(self, image_folder, weights_file):\n",
    "        self.image_folder = image_folder\n",
    "        self.weights = self.load_weights(weights_file)\n",
    "\n",
    "    def load_weights(self, weights_file):\n",
    "        weights = []\n",
    "        with open(weights_file, 'r') as file:\n",
    "            for line in file:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    pounds, ounces = map(int, line.split(','))\n",
    "                    weight = convert_to_pounds(pounds, ounces)\n",
    "                    weights.append(weight)\n",
    "        return weights\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.weights)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = os.path.join(self.image_folder, f\"{index}.png\")\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        weight = self.weights[index]\n",
    "\n",
    "        return image, weight\n",
    "\n",
    "# Define paths to the image folder and weights file\n",
    "image_folder = datadir + \"bass_images/\"\n",
    "weights_file = datadir + \"bass_weights.txt\"\n",
    "\n",
    "# Create dataset instance\n",
    "dataset = BassDataset(image_folder, weights_file)\n",
    "\n",
    "# Split the dataset into training(80) and validation(20) sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "valid_size = len(dataset) - train_size\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 16\n",
    "\n",
    "# Define the custom collate function\n",
    "def collate_fn(batch):\n",
    "    images, weights = zip(*batch)\n",
    "    transform = transforms.ToTensor()\n",
    "    images = [transform(image) for image in images]\n",
    "    images = torch.stack(images)\n",
    "    weights = torch.tensor(weights, dtype=torch.float32)\n",
    "    return images, weights\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for images, weights in train_loader:\n",
    "        images = images.to(device)\n",
    "        weights = weights.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs.squeeze(), weights)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "\n",
    "    train_loss = train_loss / len(train_dataset)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, weights in valid_loader:\n",
    "            images = images.to(device)\n",
    "            weights = weights.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs.squeeze(), weights)\n",
    "\n",
    "            valid_loss += loss.item() * images.size(0)\n",
    "\n",
    "    valid_loss = valid_loss / len(valid_dataset)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a595971",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = datadir + \"IMG_3216.png\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b474b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function uses the model to predict the weight of a given bass image.\n",
    "def predict_weight(image_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),])\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    input_image = transform(image)\n",
    "\n",
    "    # Add an extra dimension for batch\n",
    "    input_image = input_image.unsqueeze(0)\n",
    "\n",
    "    # Move the input image to the device (CPU or GPU)\n",
    "    input_image = input_image.to(device)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Perform the inference\n",
    "    with torch.no_grad():\n",
    "        output = model(input_image)\n",
    "\n",
    "    # Get the predicted weight\n",
    "    predicted_weight = output.item()\n",
    "\n",
    "    return predicted_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e148312f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.441661834716797"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_weight(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a96ace0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
